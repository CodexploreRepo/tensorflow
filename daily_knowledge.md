# Daily Knowledge

## Day 2

- Threshold Logic Unit, Perceptron, Multi-Layer Perceptron
- Backpropagation: makes predictions for a mini-batch (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), and finally tweaks the connection weights and biases to reduce the error (gradient descent step).
- Two popular choices of activation functions:
  - **Hyperbolic Tangent function**: $tanh(z) = 2σ(2z) – 1$
    - Output range: –1 to 1
    - make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.
  - **Rectified Linear Unit function** (RELU): $ReLU(z) = max(0, z)$
    - The ReLU function is continuous but unfortunately not differentiable at $z = 0$
    - It works very well and has the advantage of being fast to compute

### Memory Clearance Commands

- `tf.keras.backend.clear_session()` to reset name_counters (to uniquify autogenerated layer names) by clearing all global state managed by Keras is stored in a Keras session
  - This helps avoid clutter from old models and layers, especially when memory is limited.

### Model Compilation

- Compile defines: [Losses](https://keras.io/api/losses), [Optimizer](https://keras.io/api/optimizers), and [Metrics](https://keras.io/api/metrics)
- **Note 1**: You can compile a model as many times as you want without causing any problem to pretrained weights. &#8594; if you want to re-train the weights from stratch, you need to init model object again
- **Note 2**: You need a compiled model to train (because training uses the loss function and the optimizer). But it's not necessary to compile a model for predicting.

## Day 1

- `tensorflow` vs GPU: if you have access to a GPU, TensorFlow will automatically use it whenever possible.
- `tensorflow-metal` initialization

  - Observation: When I run a script that uses keras or tensorflow with Apple M2, it shows the initialisation message

  ```Python
  2023-10-25 23:23:46.961496: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max
  2023-10-25 23:23:46.961512: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB
  2023-10-25 23:23:46.961517: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB
  2023-10-25 23:23:46.961547: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
  2023-10-25 23:23:46.961561: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
  ```

  - Surpress the print message
    - **Level 2** is to suppress these Warnings `(W)` and Informational `(I)` messages
    - **Level 3** is that All the messages (1 - informational `(I)`, 2 - warnings `(W)` and 3- errors`(E)`) will not be logged during code execution

  ```Python
  import os
  os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

  import tensorflow as tf
  ```
